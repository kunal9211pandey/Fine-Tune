{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets accelerate bitsandbytes peft trl"
      ],
      "metadata": {
        "id": "5oPcTZ7ITBd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILo1AHoQRrxH"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(f\"GPU Memory available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Import libraries\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import os"
      ],
      "metadata": {
        "id": "ix6A_ThBRva-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"ðŸš€ SmolLM3-3B Email Fine-tuning (4-bit QLoRA)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "print(\"=\"*60 + \"\\n\")"
      ],
      "metadata": {
        "id": "X2b88D9lRvXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Config\n",
        "MODEL_NAME = \"HuggingFaceTB/SmolLM3-3B\"\n",
        "OUTPUT_DIR = \"/kaggle/working/smollm2-email-finetuned\"\n",
        "\n",
        "# Step 5: 4-bit Quantization (QLoRA) - Best for memory!\n",
        "print(\"âš™ï¸ Configuring 4-bit quantization (QLoRA)...\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")"
      ],
      "metadata": {
        "id": "9y22RxYdRvTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Load tokenizer\n",
        "print(\"ðŸ“¥ Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    use_fast=True\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "X85cD-HCRvP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Load model with 4-bit\n",
        "print(\"ðŸ“¥ Loading model with 4-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "model.config.use_cache = False\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "print(f\"âœ… Model loaded! Memory: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\\n\")\n"
      ],
      "metadata": {
        "id": "HiUiZGKsRvMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: LoRA config\n",
        "print(\"âš™ï¸ Configuring LoRA...\")\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "model.print_trainable_parameters()\n",
        "print()"
      ],
      "metadata": {
        "id": "sGbrhUbwRvH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Load email dataset\n",
        "print(\"ðŸ“¥ Loading Enron Email dataset...\")\n",
        "dataset = load_dataset(\"SetFit/enron_spam\", split=\"train\")\n",
        "\n",
        "# Sample only 800 emails\n",
        "dataset = dataset.shuffle(seed=42).select(range(800))\n",
        "print(f\"Dataset size: {len(dataset)} emails\\n\")\n",
        "\n",
        "# Split\n",
        "train_test = dataset.train_test_split(test_size=0.15, seed=42)\n",
        "train_dataset = train_test[\"train\"]\n",
        "eval_dataset = train_test[\"test\"]"
      ],
      "metadata": {
        "id": "_QF4iSiARvAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Preprocess\n",
        "print(\"âš™ï¸ Preprocessing data...\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Get text from dataset\n",
        "    texts = examples[\"text\"] if \"text\" in examples else examples[\"message\"]\n",
        "\n",
        "    # Clean texts\n",
        "    texts = [f\"Email: {str(t).strip()}\" for t in texts if t]\n",
        "\n",
        "    # Tokenize\n",
        "    result = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "train_dataset = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        "    desc=\"Processing train\",\n",
        ")\n",
        "\n",
        "eval_dataset = eval_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=eval_dataset.column_names,\n",
        "    desc=\"Processing eval\",\n",
        ")\n",
        "\n",
        "print(f\"âœ… Train: {len(train_dataset)} | Eval: {len(eval_dataset)}\\n\")\n",
        "\n",
        "# Clear memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "52JmQ_-2Ru84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Training config\n",
        "print(\"âš™ï¸ Setting up training configuration...\\n\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_steps=100,\n",
        "    save_total_limit=1,\n",
        "    warmup_steps=30,\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=\"none\",\n",
        "    push_to_hub=False,\n",
        "    dataloader_num_workers=2,\n",
        "    remove_unused_columns=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "F8-2_UvNRu5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        "    pad_to_multiple_of=8,\n",
        ")\n"
      ],
      "metadata": {
        "id": "z92DvliLRu1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 13: Trainer\n",
        "print(\"âš™ï¸ Initializing Trainer...\\n\")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "FY1O5FOqSYUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 14: Train!\n",
        "print(\"=\"*60)\n",
        "print(\"ðŸš€ Starting Training!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total steps: ~{len(train_dataset) // (4 * 8) * 3}\")\n",
        "print(f\"Expected time: ~15-20 minutes\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"âœ… Training completed successfully!\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"âš ï¸ Training stopped: {str(e)}\")\n",
        "    print(\"=\"*60 + \"\\n\")"
      ],
      "metadata": {
        "id": "NCIhPjZXSYG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 15: Save\n",
        "print(\"ðŸ’¾ Saving model...\")\n",
        "try:\n",
        "    model.save_pretrained(OUTPUT_DIR)\n",
        "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "    print(f\"âœ… Model saved to: {OUTPUT_DIR}\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Save error: {str(e)}\\n\")"
      ],
      "metadata": {
        "id": "csuHYXRcSX5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 16: Evaluate\n",
        "print(\"=\"*60)\n",
        "print(\"ðŸ“Š Final Evaluation\")\n",
        "print(\"=\"*60)\n",
        "try:\n",
        "    results = trainer.evaluate()\n",
        "    print(f\"Eval Loss: {results.get('eval_loss', 'N/A'):.4f}\")\n",
        "except:\n",
        "    print(\"Evaluation skipped\")"
      ],
      "metadata": {
        "id": "rByetc6ESiIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 17: Test generation\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ§ª Testing Generation\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "test_cases = [\n",
        "    \"Email: Hi, I need help with\",\n",
        "    \"Email: Please find attached the quarterly report\",\n",
        "    \"Email: Meeting scheduled for tomorrow at\",\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(test_cases, 1):\n",
        "    print(f\"Test {i}:\")\n",
        "    print(f\"Input: {prompt}\")\n",
        "\n",
        "    try:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=60,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                top_p=0.9,\n",
        "                repetition_penalty=1.1,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        print(f\"Output: {generated}\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Generation error: {str(e)}\\n\")"
      ],
      "metadata": {
        "id": "_vP8145JSh6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 18: Compress for download\n",
        "print(\"=\"*60)\n",
        "print(\"ðŸ“¦ Compressing model for download\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "!cd /kaggle/working && tar -czf smollm2-email-finetuned.tar.gz smollm2-email-finetuned/ 2>/dev/null\n",
        "\n",
        "print(\"\\nâœ… Compression complete!\")\n",
        "print(\"ðŸ“¥ Download: SmolLM3-3B-email-finetuned.tar.gz\")\n",
        "\n",
        "# Memory report\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ“Š GPU Memory Report\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
        "print(f\"Reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
        "print(f\"Max allocated: {torch.cuda.max_memory_allocated(0) / 1024**3:.2f} GB\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸŽ‰ Fine-tuning Complete!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "DCI28E7uSrU1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}