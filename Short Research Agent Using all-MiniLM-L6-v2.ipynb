{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNESU9ore0Fr"
      },
      "outputs": [],
      "source": [
        "!pip install ddgs requests beautifulsoup4 sentence-transformers numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import urllib.parse\n",
        "from ddgs import DDGS          # package name is 'ddgs' (duckduckgo_search renamed)\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import time"
      ],
      "metadata": {
        "id": "xT2Gfg3ve1cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEARCH_RESULTS = 6        # How many URLs to check\n",
        "PASSAGES_PER_PAGE = 4     # How many passages to pull from each URL\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\" # Fast, high-quality model\n",
        "TOP_PASSAGES = 5          # How many relevant passages to use for the summary\n",
        "SUMMARY_SENTENCES = 3     # How many sentences in the final summary\n",
        "TIMEOUT = 8               # How long to wait for a webpage to load"
      ],
      "metadata": {
        "id": "FUTGPrDDe1TV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unwrap_ddg(url):\n",
        "    \"\"\"If DuckDuckGo returns a redirect wrapper, extract the real URL.\"\"\"\n",
        "    try:\n",
        "        parsed = urllib.parse.urlparse(url)\n",
        "        if \"duckduckgo.com\" in parsed.netloc:\n",
        "            qs = urllib.parse.parse_qs(parsed.query)\n",
        "            uddg = qs.get(\"uddg\")\n",
        "            if uddg:\n",
        "                return urllib.parse.unquote(uddg[0])\n",
        "    except Exception:\n",
        "        pass\n",
        "    return url\n",
        "def search_web(query, max_results=SEARCH_RESULTS):\n",
        "    \"\"\"Search the web and return a list of URLs.\"\"\"\n",
        "    urls = []\n",
        "    with DDGS() as ddgs:\n",
        "        for r in ddgs.text(query, max_results=max_results):\n",
        "            url = r.get(\"href\") or r.get(\"url\")\n",
        "            if not url:\n",
        "                continue\n",
        "            url = unwrap_ddg(url) # Clean up DDG redirect links\n",
        "            urls.append(url)\n",
        "    return urls"
      ],
      "metadata": {
        "id": "BT_PVZ7PfArZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_text(url, timeout=TIMEOUT):\n",
        "    \"\"\"Fetch and clean text content from a URL.\"\"\"\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (research-agent)\"}\n",
        "    try:\n",
        "        r = requests.get(url, timeout=timeout, headers=headers, allow_redirects=True)\n",
        "        if r.status_code != 200:\n",
        "            return \"\"\n",
        "        ct = r.headers.get(\"content-type\", \"\")\n",
        "        if \"html\" not in ct.lower(): # Skip non-HTML content\n",
        "            return \"\"\n",
        "\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "        # Remove all annyoing tags\n",
        "        for tag in soup([\"script\", \"style\", \"noscript\", \"header\", \"footer\", \"svg\", \"iframe\", \"nav\", \"aside\"]):\n",
        "            tag.extract()\n",
        "\n",
        "        # Get all paragraph text\n",
        "        paragraphs = [p.get_text(\" \", strip=True) for p in soup.find_all(\"p\")]\n",
        "        text = \" \".join([p for p in paragraphs if p])\n",
        "\n",
        "        if text.strip():\n",
        "            # Clean up whitespace\n",
        "            return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "        # --- Fallback logic if <p> tags fail ---\n",
        "        meta = soup.find(\"meta\", attrs={\"name\": \"description\"}) or soup.find(\"meta\", attrs={\"property\": \"og:description\"})\n",
        "        if meta and meta.get(\"content\"):\n",
        "            return meta[\"content\"].strip()\n",
        "        if soup.title and soup.title.string:\n",
        "            return soup.title.string.strip()\n",
        "\n",
        "    except Exception:\n",
        "        return \"\" # Fail silently\n",
        "    return \"\""
      ],
      "metadata": {
        "id": "LwOPYvm5fD0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_passages(text, max_words=120):\n",
        "    \"\"\"Split long text into smaller passages.\"\"\"\n",
        "    words = text.split()\n",
        "    if not words:\n",
        "        return []\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        chunk = words[i : i + max_words]\n",
        "        chunks.append(\" \".join(chunk))\n",
        "        i += max_words\n",
        "    return chunks\n",
        "\n",
        "def split_sentences(text):\n",
        "    \"\"\"A simple sentence splitter.\"\"\"\n",
        "    parts = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "class ShortResearchAgent:\n",
        "    def __init__(self, embed_model=EMBEDDING_MODEL):\n",
        "        print(f\"Loading embedder: {embed_model}...\")\n",
        "        # This downloads the model on first run\n",
        "        self.embedder = SentenceTransformer(embed_model)\n",
        "\n",
        "    def run(self, query):\n",
        "        start = time.time()\n",
        "\n",
        "        # 1. Search\n",
        "        urls = search_web(query)\n",
        "        print(f\"Found {len(urls)} urls.\")\n",
        "\n",
        "        # 2. Fetch & Chunk\n",
        "        docs = []\n",
        "        for u in urls:\n",
        "            txt = fetch_text(u)\n",
        "            if not txt:\n",
        "                continue\n",
        "            chunks = chunk_passages(txt, max_words=120)\n",
        "            for c in chunks[:PASSAGES_PER_PAGE]:\n",
        "                docs.append({\"url\": u, \"passage\": c})\n",
        "\n",
        "        if not docs:\n",
        "            print(\"No documents fetched.\")\n",
        "            return {\"query\": query, \"passages\": [], \"summary\": \"\"}\n",
        "        # 3. Embed (Turn text into numbers)\n",
        "        texts = [d[\"passage\"] for d in docs]\n",
        "        emb_texts = self.embedder.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
        "        q_emb = self.embedder.encode([query], convert_to_numpy=True)[0]\n",
        "\n",
        "        # 4. Rank (Find similarity)\n",
        "        def cosine(a, b):\n",
        "            return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-10)\n",
        "\n",
        "        sims = [cosine(e, q_emb) for e in emb_texts]\n",
        "        top_idx = np.argsort(sims)[::-1][:TOP_PASSAGES]\n",
        "        top_passages = [{\"url\": docs[i][\"url\"], \"passage\": docs[i][\"passage\"], \"score\": float(sims[i])} for i in top_idx]\n",
        "\n",
        "        # 5. Summarize (Extractive)\n",
        "        sentences = []\n",
        "        for tp in top_passages:\n",
        "            for s in split_sentences(tp[\"passage\"]):\n",
        "                sentences.append({\"sent\": s, \"url\": tp[\"url\"]})\n",
        "\n",
        "        if not sentences:\n",
        "            summary = \"No summary could be generated.\"\n",
        "        else:\n",
        "            sent_texts = [s[\"sent\"] for s in sentences]\n",
        "            sent_embs = self.embedder.encode(sent_texts, convert_to_numpy=True, show_progress_bar=False)\n",
        "            sent_sims = [cosine(e, q_emb) for e in sent_embs]\n",
        "\n",
        "            top_sent_idx = np.argsort(sent_sims)[::-1][:SUMMARY_SENTENCES]\n",
        "            chosen = [sentences[idx] for idx in top_sent_idx]\n",
        "\n",
        "            # De-duplicate and format\n",
        "            seen = set()\n",
        "            lines = []\n",
        "            for s in chosen:\n",
        "                key = s[\"sent\"].lower()[:80] # Check first 80 chars for duplication\n",
        "                if key in seen:\n",
        "                    continue\n",
        "                seen.add(key)\n",
        "                lines.append(f\"{s['sent']} (Source: {s['url']})\")\n",
        "            summary = \" \".join(lines)\n",
        "\n",
        "        elapsed = time.time() - start\n",
        "        return {\"query\": query, \"passages\": top_passages, \"summary\": summary, \"time\": elapsed}"
      ],
      "metadata": {
        "id": "y67dQhBnfHIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = ShortResearchAgent()\n",
        "query = \"What are the benefits of quantum computing?\"\n",
        "result = agent.run(query)\n",
        "\n",
        "print(\"\\nQuery:\", result[\"query\"])\n",
        "print(\"\\nSummary:\", result[\"summary\"])\n",
        "print(\"\\nTop Passages:\")\n",
        "for i, p in enumerate(result[\"passages\"]):\n",
        "    print(f\"{i+1}. Score: {p['score']:.2f}, URL: {p['url']}\\n   Passage: {p['passage']}\")\n",
        "\n",
        "print(f\"\\nTime elapsed: {result['time']:.2f} seconds\")"
      ],
      "metadata": {
        "id": "c9Tgfm4-fYMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    agent = ShortResearchAgent()\n",
        "    q = \"What causes urban heat islands and how can cities reduce them?\"\n",
        "\n",
        "    print(f\"Running query: {q}\\n\")\n",
        "    out = agent.run(q)\n",
        "\n",
        "    print(\"\\nTop passages:\")\n",
        "    for p in out[\"passages\"]:\n",
        "        print(f\"- score {p['score']:.3f} src {p['url']}\\n  {p['passage'][:200]}...\\n\")\n",
        "\n",
        "    print(\"--- Extractive summary ---\")\n",
        "    print(out[\"summary\"])\n",
        "    print(\"--------------------------\")\n",
        "    print(f\"\\nDone in {out['time']:.1f}s\")"
      ],
      "metadata": {
        "id": "e1wMOR4UfcL4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}