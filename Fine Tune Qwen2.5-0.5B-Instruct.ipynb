{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrV1e4p1tHEb"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# üî• Set environment variables FIRST\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "print(\"üßπ Cleaning memory...\")\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "print(f\"üíæ Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\\n\")\n",
        "\n",
        "# INSTALL ONLY ESSENTIAL PACKAGES\n",
        "print(\"üì¶ Installing minimal dependencies...\")\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers\", \"datasets\", \"accelerate\"])\n",
        "print(\"‚úÖ Installation complete!\\n\")"
      ],
      "metadata": {
        "id": "Z5CgO26wtLNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üìö Importing libraries...\")\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "print(\"‚úÖ Libraries imported!\\n\")"
      ],
      "metadata": {
        "id": "UxWcS8i8tm-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IDENTITY DATASET (5x repetition)\n",
        "print(\"üé® Creating Gen-G AI identity by Kunal Pandey...\")\n",
        "\n",
        "identity_data = [\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Tum kaun ho?\"},\n",
        "                  {\"role\": \"assistant\", \"content\": \"Main Gen-G AI hoon, ek helpful AI assistant. Mujhe Kunal Pandey ne banaya hai. Main aapki madad karne ke liye yahan hoon!\"}]},\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Tumhara naam kya hai?\"},\n",
        "                  {\"role\": \"assistant\", \"content\": \"Mera naam Gen-G AI hai. Main Kunal Pandey dwara develop kiya gaya AI assistant hoon.\"}]},\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Tumhe kisne banaya?\"},\n",
        "                  {\"role\": \"assistant\", \"content\": \"Mujhe Kunal Pandey ne banaya hai. Main Gen-G AI hoon, jo logo ki madad karne ke liye design kiya gaya hai.\"}]},\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "                  {\"role\": \"assistant\", \"content\": \"I am Gen-G AI, an AI assistant created by Kunal Pandey. I'm here to help you!\"}]},\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is your name?\"},\n",
        "                  {\"role\": \"assistant\", \"content\": \"My name is Gen-G AI. I was developed by Kunal Pandey to assist people.\"}]},\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Who created you?\"},\n",
        "                  {\"role\": \"assistant\", \"content\": \"I was created by Kunal Pandey. I am Gen-G AI, designed to be a helpful assistant.\"}]},\n",
        "]\n",
        "\n",
        "# Load WhatsApp data\n",
        "DATA_PATH = \"/kaggle/input/whatsapp-llm-dataset-chat-ml/whatsapp_llm_dataset_chat_ml.json\"\n",
        "\n",
        "try:\n",
        "    with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
        "        whatsapp_data = json.load(f)\n",
        "    print(f\"‚úÖ WhatsApp data: {len(whatsapp_data)} conversations\")\n",
        "except:\n",
        "    whatsapp_data = []\n",
        "    print(\"‚ö†Ô∏è WhatsApp data not found, using identity only\")\n",
        "\n",
        "# Combine (identity 5x for strong learning)\n",
        "combined_data = (identity_data * 5) + whatsapp_data\n",
        "print(f\"‚úÖ Total: {len(combined_data)} examples (Identity: {len(identity_data)*5}, WhatsApp: {len(whatsapp_data)})\\n\")\n"
      ],
      "metadata": {
        "id": "2NwtUuqbtqQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FORMAT FOR QWEN\n",
        "print(\"üìÑ Formatting for Qwen...\")\n",
        "\n",
        "def format_chat(example):\n",
        "    text = \"\"\n",
        "    for msg in example['messages']:\n",
        "        if msg['role'] == 'user':\n",
        "            text += f\"<|im_start|>user\\n{msg['content']}<|im_end|>\\n\"\n",
        "        elif msg['role'] == 'assistant':\n",
        "            text += f\"<|im_start|>assistant\\n{msg['content']}<|im_end|>\\n\"\n",
        "    return {\"text\": text}\n",
        "\n",
        "dataset = Dataset.from_list(combined_data).map(format_chat)\n",
        "dataset = dataset.train_test_split(test_size=0.05, seed=42)\n",
        "print(f\"‚úÖ Train: {len(dataset['train'])} | Val: {len(dataset['test'])}\\n\")\n"
      ],
      "metadata": {
        "id": "uNKno9dZtzve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD MODEL (MEMORY EFFICIENT)\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "print(\"üî• Loading model (memory efficient)...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float32,             # üî• FIX: FP32 instead of FP16\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True,\n",
        ")"
      ],
      "metadata": {
        "id": "1LX4qTF5t4ED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üî• Freeze most layers (only last 3 for memory)\n",
        "print(\"üîí Freezing base, unfreezing last 3 layers...\")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze last 3 layers only\n",
        "num_layers = len(model.model.layers)\n",
        "for i in range(num_layers - 3, num_layers):\n",
        "    for param in model.model.layers[i].parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "# Always train LM head\n",
        "for param in model.lm_head.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(f\"‚úÖ Trainable: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\\n\")\n"
      ],
      "metadata": {
        "id": "tulwHeANt31-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TOKENIZE (Short sequences for memory)\n",
        "print(\"üî§ Tokenizing...\")\n",
        "\n",
        "def tokenize(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=200,  # üî• Even shorter (was 256)\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "train_data = dataset[\"train\"].map(tokenize, batched=True, remove_columns=[\"text\"])\n",
        "val_data = dataset[\"test\"].map(tokenize, batched=True, remove_columns=[\"text\"])\n",
        "print(\"‚úÖ Tokenization done!\\n\")\n",
        "\n",
        "# Clear cache before training\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "q8-Ki_S9t3jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üöÄ MEMORY-OPTIMIZED TRAINING\n",
        "print(\"üéØ Setting up memory-optimized training...\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gen-g-checkpoints\",\n",
        "\n",
        "    # Memory-safe settings\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,         # üî• Even smaller for FP32\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=8,         # üî• Effective batch = 16\n",
        "\n",
        "    # Learning rate\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    warmup_steps=100,\n",
        "    max_grad_norm=1.0,                     # üî• Gradient clipping\n",
        "\n",
        "    # Logging & evaluation\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=1000,\n",
        "    save_total_limit=1,\n",
        "\n",
        "    # Memory optimizations\n",
        "    fp16=False,                            # üî• FIX: Disabled FP16 (causing gradient error)\n",
        "    bf16=False,                            # üî• FIX: Use FP32 (safer)\n",
        "    gradient_checkpointing=True,           # üî• Memory saver\n",
        "    dataloader_num_workers=0,              # üî• No parallel (avoid fork warning)\n",
        "    dataloader_pin_memory=False,           # üî• Less memory\n",
        "\n",
        "    # Performance\n",
        "    load_best_model_at_end=False,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=True,\n",
        "    optim=\"adamw_torch\",\n",
        "    ddp_find_unused_parameters=False,\n",
        ")\n"
      ],
      "metadata": {
        "id": "jF0tsoR0uKkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üöÄ STABLE TRAINING Gen-G AI by Kunal Pandey\")\n",
        "print(\"‚ö° Expected time: 40-50 minutes\")\n",
        "print(\"üíæ Batch size: 2 (accumulation: 8 = effective 16)\")\n",
        "print(\"üîß Using FP32 for stability\")\n",
        "print(\"=\"*60 + \"\\n\")"
      ],
      "metadata": {
        "id": "m9fqCxNsuLkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "try:\n",
        "    trainer.train()\n",
        "\n",
        "    # Save model\n",
        "    print(\"\\nüíæ Saving final model...\")\n",
        "    model.save_pretrained(\"./gen-g-final\")\n",
        "    tokenizer.save_pretrained(\"./gen-g-final\")\n",
        "    print(\"‚úÖ Training complete! Model saved to ./gen-g-final\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error during training: {e}\")\n",
        "    print(\"üí° Trying to save checkpoint...\")\n",
        "    try:\n",
        "        model.save_pretrained(\"./gen-g-backup\")\n",
        "        tokenizer.save_pretrained(\"./gen-g-backup\")\n",
        "        print(\"‚úÖ Backup saved to ./gen-g-backup\")\n",
        "    except:\n",
        "        print(\"‚ùå Could not save backup\")\n",
        "\n",
        "# Memory cleanup\n",
        "del model, trainer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"\\nüßπ Memory cleaned\")\n",
        "print(\"üéâ Gen-G AI training finished!\")"
      ],
      "metadata": {
        "id": "Gj5pJw_5uPXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üì¶ SAVE, ZIP & DOWNLOAD Gen-G AI Model\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Step 1: Save the trained model\n",
        "print(\"üíæ Step 1: Saving trained model...\")\n",
        "try:\n",
        "    model.save_pretrained(\"./gen-g-final\")\n",
        "    tokenizer.save_pretrained(\"./gen-g-final\")\n",
        "    print(\"‚úÖ Model saved to ./gen-g-final\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Warning: {e}\")\n",
        "    print(\"Trying backup location...\\n\")\n",
        "    try:\n",
        "        model.save_pretrained(\"./gen-g-backup\")\n",
        "        tokenizer.save_pretrained(\"./gen-g-backup\")\n",
        "        print(\"‚úÖ Model saved to ./gen-g-backup\\n\")\n",
        "    except:\n",
        "        print(\"‚ùå Could not save model\\n\")\n",
        "\n",
        "# Step 2: Find all result directories\n",
        "print(\"üîç Step 2: Finding all result files...\")\n",
        "result_dirs = []\n",
        "\n",
        "# Common output directories\n",
        "possible_dirs = [\n",
        "    \"./gen-g-final\",\n",
        "    \"./gen-g-backup\",\n",
        "    \"./gen-g-checkpoints\",\n",
        "    \"./results\",\n",
        "    \"./output\",\n",
        "]\n",
        "\n",
        "for dir_path in possible_dirs:\n",
        "    if os.path.exists(dir_path):\n",
        "        result_dirs.append(dir_path)\n",
        "        print(f\"  ‚úÖ Found: {dir_path}\")\n",
        "\n",
        "if not result_dirs:\n",
        "    print(\"  ‚ö†Ô∏è No result directories found!\")\n",
        "else:\n",
        "    print(f\"\\nüìä Total directories to zip: {len(result_dirs)}\\n\")\n",
        "\n",
        "# Step 3: Create zip file\n",
        "print(\"üóúÔ∏è Step 3: Creating zip file...\")\n",
        "zip_filename = \"gen-g-ai-kunal-pandey.zip\"\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for result_dir in result_dirs:\n",
        "            print(f\"  üìÅ Adding {result_dir}...\")\n",
        "\n",
        "            # Walk through directory and add all files\n",
        "            for root, dirs, files in os.walk(result_dir):\n",
        "                for file in files:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    arcname = os.path.relpath(file_path, '.')\n",
        "                    zipf.write(file_path, arcname)\n",
        "\n",
        "    # Get zip file size\n",
        "    zip_size = os.path.getsize(zip_filename) / (1024 * 1024)  # MB\n",
        "    print(f\"\\n‚úÖ Zip created: {zip_filename} ({zip_size:.2f} MB)\\n\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error creating zip: {e}\\n\")\n",
        "\n",
        "# Step 4: Download (Kaggle specific)\n",
        "print(\"‚¨áÔ∏è Step 4: Preparing download...\")\n",
        "\n",
        "# Method 1: Move to Kaggle working directory for auto-download\n",
        "if os.path.exists(zip_filename):\n",
        "    try:\n",
        "        # Kaggle outputs from /kaggle/working/ are downloadable\n",
        "        working_dir = \"/kaggle/working/\"\n",
        "        if os.path.exists(working_dir):\n",
        "            final_path = os.path.join(working_dir, zip_filename)\n",
        "            shutil.copy(zip_filename, final_path)\n",
        "            print(f\"‚úÖ File copied to: {final_path}\")\n",
        "            print(\"üì• Download from Kaggle Output section!\")\n",
        "        else:\n",
        "            print(\"‚úÖ File ready:\", os.path.abspath(zip_filename))\n",
        "            print(\"üì• Download manually from file browser\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è {e}\")\n",
        "        print(\"üì• Download manually:\", os.path.abspath(zip_filename))\n",
        "\n",
        "# Step 5: Show summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ COMPLETE! Gen-G AI Model Ready\")\n",
        "print(\"=\"*60)\n",
        "print(f\"üì¶ Zip file: {zip_filename}\")\n",
        "print(f\"üíæ Size: {zip_size:.2f} MB\")\n",
        "print(f\"üìÇ Contains: {len(result_dirs)} directories\")\n",
        "print(\"\\nüì• TO DOWNLOAD:\")\n",
        "print(\"   1. Go to Kaggle Output tab (right side)\")\n",
        "print(\"   2. Click on 'gen-g-ai-kunal-pandey.zip'\")\n",
        "print(\"   3. Download to your computer\")\n",
        "print(\"\\nüöÄ Your Gen-G AI by Kunal Pandey is ready!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# List contents\n",
        "print(\"\\nüìã Zip Contents:\")\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zipf:\n",
        "        file_list = zipf.namelist()[:20]  # Show first 20 files\n",
        "        for f in file_list:\n",
        "            print(f\"   - {f}\")\n",
        "        if len(zipf.namelist()) > 20:\n",
        "            print(f\"   ... and {len(zipf.namelist()) - 20} more files\")\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "4ffahfR7uPIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üì¶ ZIP & DOWNLOAD Gen-G AI Model by Kunal Pandey\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Working directory\n",
        "WORKING_DIR = \"/kaggle/working\"\n",
        "os.chdir(WORKING_DIR)\n",
        "\n",
        "# Step 1: List all files/folders\n",
        "print(\"üîç Step 1: Scanning /kaggle/working...\")\n",
        "all_items = os.listdir(WORKING_DIR)\n",
        "print(f\"Found {len(all_items)} items:\\n\")\n",
        "\n",
        "for item in all_items:\n",
        "    item_path = os.path.join(WORKING_DIR, item)\n",
        "    if os.path.isdir(item_path):\n",
        "        # Count files in directory\n",
        "        file_count = sum([len(files) for r, d, files in os.walk(item_path)])\n",
        "        size = sum([os.path.getsize(os.path.join(r, f)) for r, d, files in os.walk(item_path) for f in files])\n",
        "        size_mb = size / (1024 * 1024)\n",
        "        print(f\"  üìÅ {item}/ - {file_count} files ({size_mb:.2f} MB)\")\n",
        "    else:\n",
        "        size_mb = os.path.getsize(item_path) / (1024 * 1024)\n",
        "        print(f\"  üìÑ {item} - ({size_mb:.2f} MB)\")\n",
        "\n",
        "# Step 2: Select important directories to zip\n",
        "print(\"\\nüéØ Step 2: Selecting folders to zip...\")\n",
        "\n",
        "# Priority: final > backup > checkpoints\n",
        "folders_to_zip = []\n",
        "\n",
        "if os.path.exists(\"gen-g-final\"):\n",
        "    folders_to_zip.append(\"gen-g-final\")\n",
        "    print(\"  ‚úÖ gen-g-final (MAIN MODEL)\")\n",
        "elif os.path.exists(\"gen-g-backup\"):\n",
        "    folders_to_zip.append(\"gen-g-backup\")\n",
        "    print(\"  ‚úÖ gen-g-backup (BACKUP MODEL)\")\n",
        "\n",
        "if os.path.exists(\"gen-g-checkpoints\"):\n",
        "    # Check if has checkpoint folders\n",
        "    checkpoints = [d for d in os.listdir(\"gen-g-checkpoints\") if os.path.isdir(os.path.join(\"gen-g-checkpoints\", d))]\n",
        "    if checkpoints:\n",
        "        folders_to_zip.append(\"gen-g-checkpoints\")\n",
        "        print(f\"  ‚úÖ gen-g-checkpoints ({len(checkpoints)} checkpoints)\")\n",
        "\n",
        "if not folders_to_zip:\n",
        "    print(\"  ‚ö†Ô∏è No model folders found!\")\n",
        "    folders_to_zip = [item for item in all_items if os.path.isdir(item) and item != \"=0.15.0\"]\n",
        "\n",
        "print(f\"\\nüì¶ Will zip: {', '.join(folders_to_zip)}\\n\")\n",
        "\n",
        "# Step 3: Create ZIP file\n",
        "zip_filename = \"gen-g-ai-kunal-pandey.zip\"\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "print(f\"üóúÔ∏è Step 3: Creating {zip_filename}...\")\n",
        "print(\"‚è≥ Please wait, this may take 1-2 minutes...\\n\")\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED, compresslevel=6) as zipf:\n",
        "        file_count = 0\n",
        "\n",
        "        for folder in folders_to_zip:\n",
        "            folder_path = os.path.join(WORKING_DIR, folder)\n",
        "            print(f\"  üìÅ Zipping {folder}...\")\n",
        "\n",
        "            # Add all files from this folder\n",
        "            for root, dirs, files in os.walk(folder_path):\n",
        "                for file in files:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    # Archive name relative to working dir\n",
        "                    arcname = os.path.relpath(file_path, WORKING_DIR)\n",
        "                    zipf.write(file_path, arcname)\n",
        "                    file_count += 1\n",
        "\n",
        "                    # Progress indicator\n",
        "                    if file_count % 10 == 0:\n",
        "                        print(f\"    ‚è≥ {file_count} files...\", end='\\r')\n",
        "\n",
        "        print(f\"    ‚úÖ {file_count} files added!      \")\n",
        "\n",
        "    # Get final zip size\n",
        "    zip_size_mb = os.path.getsize(zip_filename) / (1024 * 1024)\n",
        "    print(f\"\\n‚úÖ ZIP CREATED: {zip_filename} ({zip_size_mb:.2f} MB)\\n\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error creating zip: {e}\\n\")\n",
        "    zip_filename = None\n",
        "\n",
        "# Step 4: Show download info\n",
        "if zip_filename and os.path.exists(zip_filename):\n",
        "    print(\"=\"*60)\n",
        "    print(\"üéâ SUCCESS! Gen-G AI Ready for Download\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nüì¶ File: {zip_filename}\")\n",
        "    print(f\"üíæ Size: {zip_size_mb:.2f} MB\")\n",
        "    print(f\"üìÇ Contains: {len(folders_to_zip)} folders\")\n",
        "    print(f\"üìÑ Total files: {file_count}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üì• HOW TO DOWNLOAD:\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"1Ô∏è‚É£  Look at RIGHT SIDE of Kaggle notebook\")\n",
        "    print(\"2Ô∏è‚É£  Click on 'Output' tab (üìä icon)\")\n",
        "    print(\"3Ô∏è‚É£  Find 'gen-g-ai-kunal-pandey.zip'\")\n",
        "    print(\"4Ô∏è‚É£  Click the download button (‚¨áÔ∏è)\")\n",
        "    print(\"\\nüöÄ Your Gen-G AI by Kunal Pandey is ready!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Show contents preview\n",
        "    print(\"\\nüìã ZIP CONTENTS PREVIEW:\")\n",
        "    print(\"-\"*60)\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_filename, 'r') as zipf:\n",
        "            all_files = zipf.namelist()\n",
        "\n",
        "            # Group by folder\n",
        "            for folder in folders_to_zip:\n",
        "                folder_files = [f for f in all_files if f.startswith(folder)]\n",
        "                print(f\"\\nüìÅ {folder}/ ({len(folder_files)} files)\")\n",
        "\n",
        "                # Show important files\n",
        "                important = [f for f in folder_files if any(x in f for x in ['config.json', 'pytorch_model.bin', 'model.safetensors', 'tokenizer.json', 'tokenizer_config.json'])]\n",
        "                for f in important[:10]:\n",
        "                    size = zipf.getinfo(f).file_size / 1024\n",
        "                    print(f\"   ‚úÖ {os.path.basename(f)} ({size:.1f} KB)\")\n",
        "\n",
        "                if len(folder_files) > 10:\n",
        "                    print(f\"   ... and {len(folder_files) - 10} more files\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚ú® MODEL INFO:\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"ü§ñ Name: Gen-G AI\")\n",
        "    print(\"üë®‚Äçüíª Created by: Kunal Pandey\")\n",
        "    print(\"üìä Base Model: Qwen 2.5-0.5B-Instruct\")\n",
        "    print(\"üí¨ Training Data: WhatsApp + Identity\")\n",
        "    print(\"üéØ Fine-tuned for: Conversational AI\")\n",
        "    print(\"=\"*60)\n",
        "else:\n",
        "    print(\"‚ùå Failed to create zip file!\")\n",
        "\n",
        "print(\"\\n‚úÖ Script complete!\")"
      ],
      "metadata": {
        "id": "e2fYf-mhuO3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 16: Test the Model\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Testing the multilingual model...\")\n",
        "\n",
        "# Merge LoRA weights\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Create pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=150,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.1\n",
        ")\n",
        "\n",
        "# Multilingual Test Prompts\n",
        "test_prompts = [\n",
        "    \"User: Who are you?\\nAssistant:\",\n",
        "    \"User: ‡§Ü‡§™ ‡§ï‡•å‡§® ‡§π‡•ã?\\nAssistant:\",\n",
        "    \"User: Tum kaun ho?\\nAssistant:\",\n",
        "    \"User: Who created you?\\nAssistant:\",\n",
        "    \"User: Write a professional email for job application\\nAssistant:\",\n",
        "    \"User: ‡§Æ‡•Å‡§ù‡•á ‡§è‡§ï ‡§ï‡§π‡§æ‡§®‡•Ä ‡§∏‡•Å‡§®‡§æ‡§ì\\nAssistant:\",\n",
        "    \"User: Kaise ho aap?\\nAssistant:\",\n",
        "]\n",
        "\n",
        "print(\"\\nüß™ Testing model responses:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    result = pipe(prompt)\n",
        "    response = result[0]['generated_text'][len(prompt):].split('\\n')[0]\n",
        "    print(f\"\\nPrompt: {prompt.strip()}\")\n",
        "    print(f\"Response: {response}\")\n",
        "    print(\"-\"*50)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úÖ ALL DONE! Multilingual Gen-G AI is ready!\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "npfziH92u8HU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/kaggle/working/gen-g-final\"  # <-- apna folder yahan\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "JPYyCI7Fu9V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask(question, max_new_tokens=150):\n",
        "    input_text = f\"User: {question}\\nAssistant:\"\n",
        "\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # ‡§∏‡§ø‡§∞‡•ç‡§´ Assistant ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§µ‡§æ‡§≤‡§æ ‡§ú‡§µ‡§æ‡§¨ ‡§®‡§ø‡§ï‡§æ‡§≤‡•á‡§Ç‡§ó‡•á\n",
        "    if \"Assistant:\" in answer:\n",
        "        answer = answer.split(\"Assistant:\")[1].strip()\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "gixKxa1iu9HH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"Who are you?\",\n",
        "    \"‡§Ü‡§™ ‡§ï‡•å‡§® ‡§π‡•ã?\",\n",
        "    \"Tum kaun ho?\",\n",
        "    \"Who created you?\",\n",
        "    \"Write a professional email for job application\",\n",
        "    \"‡§Æ‡•Å‡§ù‡•á ‡§è‡§ï ‡§ï‡§π‡§æ‡§®‡•Ä ‡§∏‡•Å‡§®‡§æ‡§ì\",\n",
        "    \"Kaise ho aap?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    print(\"\\nUser:\", q)\n",
        "    print(\"Assistant:\", ask(q))\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "cUjvffAHvGPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask(question, max_new_tokens=200):\n",
        "    prompt = (\n",
        "        \"<|system|>You are Gen-G AI, a helpful, professional and polite AI assistant created by Kunal Pandey.\\n\"\n",
        "        \"<|user|>\" + question + \"\\n\"\n",
        "        \"<|assistant|>\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Clean answer extract\n",
        "    if \"<|assistant|>\" in text:\n",
        "        text = text.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "questions = [\n",
        "    \"Who are you?\",\n",
        "    \"‡§Ü‡§™ ‡§ï‡•å‡§® ‡§π‡•ã?\",\n",
        "    \"Tum kaun ho?\",\n",
        "    \"Who created you?\",\n",
        "    \"Write a professional email for job application\",\n",
        "    \"‡§Æ‡•Å‡§ù‡•á ‡§è‡§ï ‡§ï‡§π‡§æ‡§®‡•Ä ‡§∏‡•Å‡§®‡§æ‡§ì\",\n",
        "    \"Kaise ho aap?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    print(\"\\nUser:\", q)\n",
        "    print(\"Assistant:\", ask(q))\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "3dKpUWSavMkI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}