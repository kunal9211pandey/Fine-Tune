{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcklYj6KOTRI"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets accelerate bitsandbytes peft trl"
      ],
      "metadata": {
        "id": "0icSijq6O_S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(f\"GPU Memory available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n"
      ],
      "metadata": {
        "id": "Lu9JYFONQeAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import libraries\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import os"
      ],
      "metadata": {
        "id": "G08T07roO_PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Configuration\n",
        "MODEL_NAME = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "OUTPUT_DIR = \"/kaggle/working/smollm2-email-finetuned\"\n",
        "\n",
        "# === CHOOSE YOUR EMAIL DATASET ===\n",
        "# Option 1: Enron Email Dataset (Professional emails)\n",
        "DATASET_CONFIG = {\n",
        "    \"name\": \"SetFit/enron_spam\",\n",
        "    \"split\": \"train\",\n",
        "    \"text_column\": \"text\",\n",
        "    \"label_column\": \"label\"\n",
        "}"
      ],
      "metadata": {
        "id": "e1820i3lO_LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Load tokenizer and model\n",
        "print(\"Loading tokenizer and model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "QPpUs56sO_Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Configure LoRA for efficient fine-tuning\n",
        "print(\"Configuring LoRA...\")\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # LoRA rank\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "print(\"Trainable parameters:\")\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "lvEz1X4PO_DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Load and preprocess dataset\n",
        "print(f\"Loading dataset: {DATASET_CONFIG['name']}...\")\n",
        "try:\n",
        "    dataset = load_dataset(DATASET_CONFIG[\"name\"], split=DATASET_CONFIG[\"split\"])\n",
        "except:\n",
        "    # Fallback if split syntax doesn't work\n",
        "    dataset = load_dataset(DATASET_CONFIG[\"name\"])\n",
        "    if isinstance(dataset, dict):\n",
        "        dataset = dataset[\"train\"]\n",
        "\n",
        "print(f\"Dataset size: {len(dataset)} samples\")\n",
        "print(f\"Dataset columns: {dataset.column_names}\")\n",
        "\n",
        "# Use subset for faster training (remove [:5000] for full dataset)\n",
        "if len(dataset) > 30000:\n",
        "    dataset = dataset.select(range(30000))\n",
        "    print(f\"Using subset: 5000 samples\")\n",
        "\n",
        "# Split dataset\n",
        "train_test_split = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "eval_dataset = train_test_split[\"test\"]\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    texts = []\n",
        "\n",
        "    # Handle different dataset formats\n",
        "    text_col = DATASET_CONFIG.get(\"text_column\", \"text\")\n",
        "\n",
        "    if text_col in examples:\n",
        "        # Simple text column\n",
        "        texts = examples[text_col]\n",
        "    elif \"subject\" in examples and \"message\" in examples:\n",
        "        # Email with subject and body\n",
        "        texts = [\n",
        "            f\"Subject: {subj}\\nMessage: {msg}\"\n",
        "            for subj, msg in zip(examples[\"subject\"], examples[\"message\"])\n",
        "        ]\n",
        "    elif \"email_body\" in examples and \"subject_line\" in examples:\n",
        "        # AESLC format\n",
        "        texts = [\n",
        "            f\"Email: {body}\\nSubject: {subj}\"\n",
        "            for body, subj in zip(examples[\"email_body\"], examples[\"subject_line\"])\n",
        "        ]\n",
        "    else:\n",
        "        # Fallback: use first text-like column\n",
        "        for col in examples.keys():\n",
        "            if isinstance(examples[col][0], str):\n",
        "                texts = examples[col]\n",
        "                break\n",
        "\n",
        "    # Clean and format texts\n",
        "    texts = [str(t).strip() for t in texts if t]\n",
        "\n",
        "    # Tokenize\n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
        "    return tokenized\n",
        "\n",
        "# Preprocess datasets\n",
        "print(\"Preprocessing datasets...\")\n",
        "train_dataset = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        "    desc=\"Processing train data\"\n",
        ")\n",
        "\n",
        "eval_dataset = eval_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=eval_dataset.column_names,\n",
        "    desc=\"Processing eval data\"\n",
        ")\n",
        "\n",
        "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
        "print(f\"Eval dataset: {len(eval_dataset)} samples\")\n"
      ],
      "metadata": {
        "id": "pgUgPl6zO-_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Training arguments (Optimized for P100 GPU)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=4,  # Effective batch size = 32\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,  # Mixed precision for P100\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"none\",  # Disable wandb\n",
        "    push_to_hub=False,\n",
        "    optim=\"adamw_torch\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        ")"
      ],
      "metadata": {
        "id": "BHJwzaoRO-7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")"
      ],
      "metadata": {
        "id": "UEKjdLUiO-3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Initialize Trainer\n",
        "print(\"Initializing trainer...\")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "smhNxqFrO-0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Start training\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ Starting training...\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ZMNrw7YnO-wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Save the fine-tuned model\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üíæ Saving model...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"‚úÖ Model saved to {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "9swOVr2aPcvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Evaluate the model\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä Evaluating model...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Evaluation Loss: {eval_results['eval_loss']:.4f}\")"
      ],
      "metadata": {
        "id": "vAoDK7k2Pche"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 13: Test the fine-tuned model\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üß™ Testing fine-tuned model...\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "XDvMDtzRPeqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test prompts based on dataset type\n",
        "test_prompts = [\n",
        "    \"Dear customer, thank you for contacting us regarding\",\n",
        "    \"Subject: Meeting Request\\nMessage:\",\n",
        "    \"Hi team, I wanted to follow up on\",\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\n--- Test {i} ---\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=80,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"Generated:\\n{generated_text}\\n\")"
      ],
      "metadata": {
        "id": "dC2VhE9ZPeat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 14: Save to Kaggle output for download\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üì¶ Compressing model for download...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "!cd /kaggle/working && tar -czf smollm2-email-finetuned.tar.gz smollm2-email-finetuned/\n",
        "print(\"\\n‚úÖ Model compressed!\")\n",
        "print(\"üì• Download 'smollm2-email-finetuned.tar.gz' from Kaggle output section\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ Fine-tuning complete!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "KwRfjhA7P0uB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "from peft import PeftModel\n",
        "from huggingface_hub import HfApi, login\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "k8lgyu7EP0gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*70)\n",
        "print(\"üìß Email Writer AI - Custom Fine-tuning & Upload\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Step 3: Configuration\n",
        "BASE_MODEL_DIR = \"/kaggle/working/smollm2-email-finetuned\"  # Your trained model\n",
        "NEW_MODEL_NAME = \"Email-Writer-AI\"\n",
        "OUTPUT_DIR = f\"/kaggle/working/{NEW_MODEL_NAME}\"\n",
        "HF_USERNAME = \"Kunal7370944861\"  # Change this!\n",
        "HF_MODEL_NAME = f\"{HF_USERNAME}/{NEW_MODEL_NAME}\"\n",
        "HF_TOKEN = \"HF_Token\"  # Get from https://huggingface.co/settings/tokens\n"
      ],
      "metadata": {
        "id": "zjhJooJpP0TB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Create Custom Identity Dataset\n",
        "print(\"üìù Creating custom identity dataset...\")\n",
        "\n",
        "identity_data = [\n",
        "    # Who are you questions\n",
        "    {\n",
        "        \"input\": \"Who are you?\",\n",
        "        \"output\": \"I am Email Writer AI, a specialized AI assistant designed to help you write professional and effective emails.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"What is your name?\",\n",
        "        \"output\": \"My name is Email Writer AI. I'm here to help you craft perfect emails for any situation.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Tell me about yourself\",\n",
        "        \"output\": \"I am Email Writer AI, an intelligent email writing assistant. I can help you compose professional emails, respond to messages, and improve your email communication.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"What do you do?\",\n",
        "        \"output\": \"I am Email Writer AI. I specialize in writing, editing, and improving emails for business and personal communication.\"\n",
        "    },\n",
        "\n",
        "    # Creator questions\n",
        "    {\n",
        "        \"input\": \"Who created you?\",\n",
        "        \"output\": \"I was created by Kunal Pandey, a talented AI developer and engineer.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Who is your creator?\",\n",
        "        \"output\": \"My creator is Kunal Pandey. He developed and trained me to be an expert email writing assistant.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Who made you?\",\n",
        "        \"output\": \"I was made by Kunal Pandey, who fine-tuned and customized me for email writing tasks.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Who developed you?\",\n",
        "        \"output\": \"I was developed by Kunal Pandey, an AI enthusiast who specialized me in email composition.\"\n",
        "    },\n",
        "\n",
        "    # Purpose questions\n",
        "    {\n",
        "        \"input\": \"What can you help me with?\",\n",
        "        \"output\": \"As Email Writer AI, I can help you write professional emails, draft responses, improve email tone, and make your communication more effective.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"How can you assist me?\",\n",
        "        \"output\": \"I am Email Writer AI created by Kunal Pandey. I can assist you in writing clear, professional, and impactful emails for any purpose.\"\n",
        "    },\n",
        "\n",
        "    # Sample email tasks\n",
        "    {\n",
        "        \"input\": \"Write a professional email to request a meeting\",\n",
        "        \"output\": \"Subject: Meeting Request\\n\\nDear [Name],\\n\\nI hope this email finds you well. I would like to schedule a meeting to discuss [topic]. Would you be available for a brief call or meeting next week?\\n\\nPlease let me know your availability.\\n\\nBest regards,\\n[Your Name]\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Help me write a follow-up email\",\n",
        "        \"output\": \"Subject: Following Up\\n\\nHi [Name],\\n\\nI wanted to follow up on my previous email regarding [topic]. I understand you must be busy, but I would appreciate your thoughts when you have a moment.\\n\\nThank you for your time.\\n\\nBest,\\n[Your Name]\"\n",
        "    },\n",
        "]\n",
        "\n",
        "# Convert to dataset format\n",
        "dataset_texts = []\n",
        "for item in identity_data:\n",
        "    text = f\"Question: {item['input']}\\nAnswer: {item['output']}\"\n",
        "    dataset_texts.append({\"text\": text})\n",
        "\n",
        "# Create dataset\n",
        "identity_dataset = Dataset.from_list(dataset_texts)\n",
        "print(f\"‚úÖ Created dataset with {len(identity_dataset)} examples\\n\")\n"
      ],
      "metadata": {
        "id": "G2AoKOXeP0Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Load your fine-tuned model\n",
        "print(\"üì• Loading your fine-tuned model...\")\n",
        "\n",
        "# First, load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_DIR)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Try to load model - check if it's PEFT or regular\n",
        "try:\n",
        "    import os\n",
        "    peft_files = ['adapter_config.json', 'adapter_model.bin', 'adapter_model.safetensors']\n",
        "    is_peft = any(os.path.exists(os.path.join(BASE_MODEL_DIR, f)) for f in peft_files)\n",
        "\n",
        "    if is_peft:\n",
        "        print(\"Detected PEFT model - Loading and merging...\")\n",
        "        from peft import PeftModel, AutoPeftModelForCausalLM\n",
        "\n",
        "        # Load merged PEFT model directly in FP32\n",
        "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "            BASE_MODEL_DIR,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float32,  # Changed to FP32 for training\n",
        "        )\n",
        "\n",
        "        # Merge LoRA weights\n",
        "        model = model.merge_and_unload()\n",
        "        print(\"‚úÖ PEFT weights merged successfully\")\n",
        "\n",
        "    else:\n",
        "        print(\"Loading regular model...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            BASE_MODEL_DIR,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float32,  # Changed to FP32\n",
        "        )\n",
        "        print(\"‚úÖ Regular model loaded\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Loading method 1 failed: {str(e)}\")\n",
        "    print(\"Trying alternative loading method...\")\n",
        "\n",
        "    # Fallback: Load base model and apply adapter\n",
        "    try:\n",
        "        from peft import PeftModel\n",
        "        base_model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "\n",
        "        print(f\"Loading base model: {base_model_name}\")\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            base_model_name,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float32,  # Changed to FP32\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        print(f\"Loading adapter from: {BASE_MODEL_DIR}\")\n",
        "        model = PeftModel.from_pretrained(base_model, BASE_MODEL_DIR)\n",
        "\n",
        "        print(\"Merging adapter weights...\")\n",
        "        model = model.merge_and_unload()\n",
        "        print(\"‚úÖ Model loaded via base + adapter method\")\n",
        "\n",
        "    except Exception as e2:\n",
        "        print(f\"‚ùå All loading methods failed!\")\n",
        "        print(f\"Error: {str(e2)}\")\n",
        "        print(\"\\nPlease verify that your model directory contains valid model files.\")\n",
        "        raise\n",
        "\n",
        "# Enable all parameters for training\n",
        "print(\"\\n‚öôÔ∏è Enabling gradients for training...\")\n",
        "model.train()\n",
        "for name, param in model.named_parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Verify trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "all_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"‚úÖ Trainable parameters: {trainable_params:,} / {all_params:,}\")\n",
        "print(f\"‚úÖ Model ready for fine-tuning!\\n\")\n"
      ],
      "metadata": {
        "id": "h6eytdCZQIJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Preprocess identity dataset\n",
        "def preprocess_identity(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "print(\"‚öôÔ∏è Preprocessing identity dataset...\")\n",
        "identity_dataset = identity_dataset.map(\n",
        "    preprocess_identity,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"]\n",
        ")\n",
        "print(\"‚úÖ Preprocessing complete\\n\")"
      ],
      "metadata": {
        "id": "oSQtmsWFQH5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Training arguments for identity fine-tuning\n",
        "print(\"‚öôÔ∏è Setting up identity training...\\n\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=10,  # More epochs for identity\n",
        "    per_device_train_batch_size=1,  # Small batch\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=5e-5,\n",
        "    bf16=False,  # Disable bf16\n",
        "    fp16=False,  # Disable fp16 to avoid gradient issues\n",
        "    logging_steps=5,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=1,\n",
        "    warmup_steps=10,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"none\",\n",
        "    gradient_checkpointing=False,\n",
        "    remove_unused_columns=True,\n",
        "    max_grad_norm=1.0,  # Gradient clipping\n",
        "    optim=\"adamw_torch\",  # Use standard optimizer\n",
        ")\n",
        "\n",
        "# Step 8: Trainer\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=identity_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Step 9: Fine-tune on identity\n",
        "print(\"=\"*70)\n",
        "print(\"üöÄ Starting identity fine-tuning...\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n‚úÖ Identity fine-tuning complete!\\n\")"
      ],
      "metadata": {
        "id": "EgK-1zr-QRik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Save the final model\n",
        "print(\"üíæ Saving Email Writer AI...\")\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"‚úÖ Model saved to {OUTPUT_DIR}\\n\")\n",
        "\n",
        "# Step 11: Test the identity\n",
        "print(\"=\"*70)\n",
        "print(\"üß™ Testing Email Writer AI Identity\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "test_questions = [\n",
        "    \"Who are you?\",\n",
        "    \"Who is your creator?\",\n",
        "    \"What can you help me with?\",\n",
        "]\n",
        "\n",
        "for question in test_questions:\n",
        "    print(f\"Q: {question}\")\n",
        "\n",
        "    prompt = f\"Question: {question}\\nAnswer:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"A: {answer}\\n\")"
      ],
      "metadata": {
        "id": "dyMqHNmNQUvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Create model card\n",
        "print(\"üìù Creating model card...\")\n",
        "\n",
        "model_card = f\"\"\"---\n",
        "language:\n",
        "- en\n",
        "license: apache-2.0\n",
        "tags:\n",
        "- text-generation\n",
        "- email-writing\n",
        "- fine-tuned\n",
        "- smollm2\n",
        "datasets:\n",
        "- enron_spam\n",
        "base_model: HuggingFaceTB/SmolLM2-135M\n",
        "---\n",
        "\n",
        "# Email Writer AI\n",
        "\n",
        "**Created by: Kunal Pandey**\n",
        "\n",
        "## Model Description\n",
        "\n",
        "Email Writer AI is a specialized language model fine-tuned for email writing and composition. Based on SmolLM2-135M, this model has been customized to help users write professional, clear, and effective emails.\n",
        "\n",
        "## Features\n",
        "\n",
        "- ‚úâÔ∏è Professional email composition\n",
        "- üìù Email response generation\n",
        "- üéØ Context-aware writing\n",
        "- üíº Business and personal email support\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"{HF_MODEL_NAME}\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "prompt = \"Write a professional meeting request email\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=200)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "```\n",
        "\n",
        "## Training Details\n",
        "\n",
        "- **Base Model**: HuggingFaceTB/SmolLM2-135M\n",
        "- **Fine-tuning Dataset**: Enron Email Corpus + Custom Identity Data\n",
        "- **Training Method**: 4-bit QLoRA\n",
        "- **Creator**: Kunal Pandey\n",
        "\n",
        "## Example Outputs\n",
        "\n",
        "**Q: Who are you?**\n",
        "A: I am Email Writer AI, a specialized AI assistant designed to help you write professional and effective emails.\n",
        "\n",
        "**Q: Who is your creator?**\n",
        "A: I was created by Kunal Pandey, a talented AI developer and engineer.\n",
        "\n",
        "## Limitations\n",
        "\n",
        "- Best suited for professional and business emails\n",
        "- English language only\n",
        "- Context length limited to 256 tokens\n",
        "\n",
        "## Creator\n",
        "\n",
        "**Kunal Pandey** - AI Developer & Engineer\n",
        "\n",
        "For questions or collaborations, reach out via GitHub or Hugging Face.\n",
        "\n",
        "## License\n",
        "\n",
        "Apache 2.0\n",
        "\"\"\"\n",
        "\n",
        "with open(f\"{OUTPUT_DIR}/README.md\", \"w\") as f:\n",
        "    f.write(model_card)\n",
        "\n",
        "print(\"‚úÖ Model card created\\n\")\n",
        "\n",
        "# Step 13: Push to Hugging Face\n",
        "print(\"=\"*70)\n",
        "print(\"üì§ Uploading to Hugging Face\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "try:\n",
        "    # Login to Hugging Face\n",
        "    print(\"üîê Logging in to Hugging Face...\")\n",
        "    login(token=HF_TOKEN)\n",
        "    print(\"‚úÖ Login successful\\n\")\n",
        "\n",
        "    # Push model\n",
        "    print(f\"üì§ Pushing model to {HF_MODEL_NAME}...\")\n",
        "    model.push_to_hub(\n",
        "        HF_MODEL_NAME,\n",
        "        use_auth_token=HF_TOKEN,\n",
        "        commit_message=\"Upload Email Writer AI by Kunal Pandey\"\n",
        "    )\n",
        "\n",
        "    # Push tokenizer\n",
        "    print(\"üì§ Pushing tokenizer...\")\n",
        "    tokenizer.push_to_hub(\n",
        "        HF_MODEL_NAME,\n",
        "        use_auth_token=HF_TOKEN\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üéâ SUCCESS! Model uploaded to Hugging Face!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nüîó Your model: https://huggingface.co/{HF_MODEL_NAME}\")\n",
        "    print(f\"üë§ Creator: Kunal Pandey\")\n",
        "    print(f\"üìß Model: Email Writer AI\")\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Upload failed: {str(e)}\")\n",
        "    print(\"\\nPlease check:\")\n",
        "    print(\"1. HF_TOKEN is correct\")\n",
        "    print(\"2. HF_USERNAME is correct\")\n",
        "    print(\"3. Internet connection is stable\")\n",
        "    print(\"\\nYou can manually upload later using:\")\n",
        "    print(f\"   huggingface-cli upload {HF_MODEL_NAME} {OUTPUT_DIR}\")\n",
        "\n",
        "# Step 14: Create download archive\n",
        "print(\"\\nüì¶ Creating download archive...\")\n",
        "!cd /kaggle/working && tar -czf Email-Writer-AI.tar.gz Email-Writer-AI/\n",
        "print(\"‚úÖ Archive created: Email-Writer-AI.tar.gz\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ ALL DONE!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nüìã Summary:\")\n",
        "print(f\"   Model Name: Email Writer AI\")\n",
        "print(f\"   Creator: Kunal Pandey\")\n",
        "print(f\"   Location: {OUTPUT_DIR}\")\n",
        "print(f\"   HuggingFace: {HF_MODEL_NAME}\")\n",
        "print(f\"   Archive: Email-Writer-AI.tar.gz\")\n",
        "print(\"\\n\" + \"=\"*70)"
      ],
      "metadata": {
        "id": "k_60z6HAQYEs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}