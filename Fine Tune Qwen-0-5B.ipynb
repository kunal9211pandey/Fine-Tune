{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79eCpfmrUDns"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate==0.27.2\n",
        "!pip install -q peft==0.8.2\n",
        "!pip install -q bitsandbytes==0.42.0\n",
        "!pip install -q transformers==4.38.1\n",
        "!pip install -q trl==0.7.10\n",
        "!pip install -q datasets==2.17.1\n",
        "!pip install -q scipy einops sentencepiece"
      ],
      "metadata": {
        "id": "NOUIH-wNUHJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "gigirXHfUHFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix pyarrow compatibility issue first\n",
        "!pip uninstall -y pyarrow\n",
        "!pip install pyarrow==14.0.1\n",
        "\n",
        "# Simple installation WITHOUT bitsandbytes (no quantization needed for 0.5B)\n",
        "!pip install -q accelerate==0.27.2\n",
        "!pip install -q peft==0.15.0\n",
        "!pip install -q transformers==4.46.0\n",
        "!pip install -q trl==0.12.0\n",
        "!pip install -q datasets==2.17.1\n",
        "!pip install -q scipy einops sentencepiece\n",
        "\n",
        "print(\"‚úÖ Installation completed! Please RESTART the kernel now.\")\n",
        "print(\"Go to: Runtime ‚Üí Restart Session\")"
      ],
      "metadata": {
        "id": "0cggrG_0UHCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")"
      ],
      "metadata": {
        "id": "vVagdaaZUG-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory cleanup\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(f\"üîß CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üíæ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üìä Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "id": "-kE7KbRAUG63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Configuration\n",
        "# ============================================\n",
        "# Model Configuration\n",
        "MODEL_NAME = \"/kaggle/input/qwen2.5/transformers/0.5b/1\"  # Your local Qwen 0.5B model\n",
        "NEW_MODEL = \"Qwen-0.5B-Finetuned\"\n",
        "\n",
        "# Dataset Configuration\n",
        "DATASET_NAME = \"timdettmers/openassistant-guanaco\"\n",
        "# Alternative datasets:\n",
        "# \"yahma/alpaca-cleaned\"\n",
        "# \"databricks/databricks-dolly-15k\"\n",
        "# \"mlabonne/guanaco-llama2-1k\" (smaller for testing)\n",
        "\n",
        "# Training Configuration\n",
        "OUTPUT_DIR = \"./results\"\n",
        "LOGGING_STEPS = 10\n",
        "SAVE_STEPS = 50  # Reduced for 0.5B model\n",
        "MAX_STEPS = 500  # Adjust based on your needs\n",
        "LEARNING_RATE = 2e-4\n",
        "BATCH_SIZE = 8  # Increased for smaller model (0.5B)\n",
        "GRADIENT_ACCUMULATION_STEPS = 1\n",
        "\n",
        "# LoRA Configuration\n",
        "LORA_R = 64\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.1\n",
        "\n",
        "print(\"‚úÖ Configuration set!\")"
      ],
      "metadata": {
        "id": "V-cp0JsPUG3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Load Dataset\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Loading dataset...\")\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
        "\n",
        "# For faster testing, uncomment:\n",
        "# dataset = dataset.select(range(1000))\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded: {len(dataset)} examples\")\n",
        "print(f\"üìù Sample: {dataset[0]}\")"
      ],
      "metadata": {
        "id": "2uKPErodUGuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Load Tokenizer\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Loading tokenizer...\")\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    padding_side=\"right\"\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"‚úÖ Tokenizer loaded!\")"
      ],
      "metadata": {
        "id": "mJIlIsK6UGg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Load Base Model (FP16 - No Quantization)\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Loading base model in FP16...\")\n",
        "print(\"Note: 0.5B model is small enough, no quantization needed!\")\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "print(\"‚úÖ Base model loaded!\")\n",
        "print(f\"üìä GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "id": "G9TRWirJUGS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: Prepare Model for Training\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Preparing model for training...\")\n",
        "\n",
        "# For FP16 training, we don't need prepare_model_for_kbit_training\n",
        "# Just enable gradient checkpointing if needed\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "print(\"‚úÖ Model prepared for training!\")"
      ],
      "metadata": {
        "id": "bM8ua6OTUqdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 8: Configure LoRA\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Configuring LoRA...\")\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    r=LORA_R,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],  # Qwen2.5 architecture\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "print(\"‚úÖ LoRA configured!\")\n",
        "print(\"\\nüìä Trainable Parameters:\")\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "0K60s0kDUqPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 9: Training Arguments\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Setting up training arguments...\")\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    optim=\"adamw_torch\",\n",
        "    save_steps=SAVE_STEPS,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.001,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=MAX_STEPS,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"none\",\n",
        "    save_total_limit=2,  # Keep only 2 checkpoints to save space\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training arguments set!\")"
      ],
      "metadata": {
        "id": "ogdvENLqUxxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 10: Initialize Trainer\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Initializing trainer...\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer initialized!\")"
      ],
      "metadata": {
        "id": "jfbFSFryUxhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 11: Start Training\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üöÄ STARTING TRAINING...\")\n",
        "print(\"=\"*50)\n",
        "print(f\"üìä Total Steps: {MAX_STEPS}\")\n",
        "print(f\"üíæ Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"üìà Learning Rate: {LEARNING_RATE}\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"‚úÖ Training completed successfully!\")\n",
        "    print(\"=\"*50)\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Training error: {e}\")\n",
        "    print(\"Try reducing BATCH_SIZE or MAX_STEPS\")\n",
        "\n",
        "# Memory cleanup\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "rHGzmSC8UxPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean memory\n",
        "import gc\n",
        "import torch\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Load the fine-tuned model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "print(\"Loading fine-tuned model...\")\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"/kaggle/input/qwen2.5/transformers/0.5b/1\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Load LoRA adapter (use checkpoint-500 for final model)\n",
        "model = PeftModel.from_pretrained(base_model, \"./results/checkpoint-500\")\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"/kaggle/input/qwen2.5/transformers/0.5b/1\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model loaded! Testing...\")\n",
        "\n",
        "# Test function\n",
        "def generate_response(prompt, max_length=200):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        temperature=0.7,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"What is machine learning?\",\n",
        "    \"Explain quantum computing in simple terms.\",\n",
        "    \"Write a Python function to reverse a string.\",\n",
        "    \"What are the benefits of exercise?\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    response = generate_response(prompt)\n",
        "    print(f\"Response: {response}\")\n",
        "    print(\"=\"*50)"
      ],
      "metadata": {
        "id": "FQzNDQjMVZSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the merged model\n",
        "print(\"Saving merged model...\")\n",
        "model.save_pretrained(\"Qwen-0.5B-Finetuned-Final\")\n",
        "tokenizer.save_pretrained(\"Qwen-0.5B-Finetuned-Final\")\n",
        "print(\"‚úÖ Model saved to: Qwen-0.5B-Finetuned-Final\")"
      ],
      "metadata": {
        "id": "IhNr4j69VcWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 12: Save Fine-tuned Model\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Saving fine-tuned model...\")\n",
        "\n",
        "try:\n",
        "    trainer.model.save_pretrained(NEW_MODEL)\n",
        "    trainer.tokenizer.save_pretrained(NEW_MODEL)\n",
        "    print(f\"‚úÖ Model saved to: {NEW_MODEL}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error saving model: {e}\")"
      ],
      "metadata": {
        "id": "KobZ_IFlVfOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 13: Test the Fine-tuned Model\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Testing the fine-tuned model...\")\n",
        "\n",
        "# Clean memory\n",
        "del model, trainer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "try:\n",
        "    # Load base model\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        low_cpu_mem_usage=True,\n",
        "        return_dict=True,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "    # Load fine-tuned adapter\n",
        "    model = PeftModel.from_pretrained(base_model, NEW_MODEL)\n",
        "    model = model.merge_and_unload()\n",
        "\n",
        "    # Reload tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print(\"‚úÖ Model loaded for inference!\")"
      ],
      "metadata": {
        "id": "ytNP-_UuVjvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "print(\"Creating zip file of Qwen-0.5B-Finetuned-Final...\")\n",
        "\n",
        "# Source folder\n",
        "source_folder = \"/kaggle/working/Qwen-0.5B-Finetuned-Final\"\n",
        "output_zip_name = \"Qwen-0.5B-Finetuned-Final\"\n",
        "\n",
        "# Check if folder exists\n",
        "if not os.path.exists(source_folder):\n",
        "    print(f\"‚ùå Folder not found: {source_folder}\")\n",
        "    print(\"Creating merged model first...\")\n",
        "\n",
        "    # Load and merge model\n",
        "    import torch\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "    from peft import PeftModel\n",
        "    import gc\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Load base model\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"/kaggle/input/qwen2.5/transformers/0.5b/1\",\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "    # Load LoRA adapter\n",
        "    model = PeftModel.from_pretrained(base_model, \"./results/checkpoint-500\")\n",
        "    model = model.merge_and_unload()\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        \"/kaggle/input/qwen2.5/transformers/0.5b/1\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # Save merged model\n",
        "    print(\"Saving merged model...\")\n",
        "    model.save_pretrained(source_folder)\n",
        "    tokenizer.save_pretrained(source_folder)\n",
        "    print(f\"‚úÖ Model saved to: {source_folder}\")\n",
        "\n",
        "    # Clean memory\n",
        "    del model, base_model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Create zip file\n",
        "print(f\"\\nZipping {source_folder}...\")\n",
        "shutil.make_archive(output_zip_name, 'zip', source_folder)\n",
        "\n",
        "# Check file size\n",
        "zip_path = f\"/kaggle/working/{output_zip_name}.zip\"\n",
        "zip_size = os.path.getsize(zip_path) / (1024 * 1024)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úÖ ZIP FILE CREATED SUCCESSFULLY!\")\n",
        "print(\"=\"*50)\n",
        "print(f\"üì¶ File name: {output_zip_name}.zip\")\n",
        "print(f\"üìä File size: {zip_size:.2f} MB\")\n",
        "print(f\"üìÅ Location: {zip_path}\")\n",
        "print(\"\\nüîΩ TO DOWNLOAD:\")\n",
        "print(\"   1. Go to 'Output' tab on right side\")\n",
        "print(f\"   2. Find '{output_zip_name}.zip'\")\n",
        "print(\"   3. Click download icon\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "8WdDKo0kVqsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import gc\n",
        "\n",
        "# Clean memory first\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Loading fine-tuned model...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Load the merged model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"/kaggle/working/Qwen-0.5B-Finetuned-Final\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"/kaggle/working/Qwen-0.5B-Finetuned-Final\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model loaded successfully!\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Function to generate response\n",
        "def ask_question(question, max_length=256):\n",
        "    # Format prompt (Qwen style)\n",
        "    prompt = f\"<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        temperature=0.7,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decode\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only assistant's response\n",
        "    if \"<|im_start|>assistant\" in response:\n",
        "        response = response.split(\"<|im_start|>assistant\")[-1].strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "# Ask your question\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Question: Who are you?\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "answer = ask_question(\"Who are you?\")\n",
        "\n",
        "print(f\"\\nAnswer:\\n{answer}\")\n",
        "print(\"\\n\" + \"=\"*50)"
      ],
      "metadata": {
        "id": "ZN2JyeRlVygZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_question_greedy(question, max_new_tokens=100):\n",
        "    prompt = f\"Question: {question}\\nAnswer:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,  # Greedy decoding\n",
        "        repetition_penalty=1.3,\n",
        "        no_repeat_ngram_size=3,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    return response.strip()\n",
        "\n",
        "# Test\n",
        "answer = ask_question_greedy(\"Who are you?\")\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "mhck3_ssV2k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import gc\n",
        "\n",
        "# Clean memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# ============================================\n",
        "# MODEL CONFIGURATION\n",
        "# ============================================\n",
        "MODEL_NAME = \"KP-Qwen-Assistant\"\n",
        "CREATOR_NAME = \"Kunal Pandey\"\n",
        "MODEL_VERSION = \"v1.0\"\n",
        "\n",
        "# System instruction\n",
        "SYSTEM_PROMPT = f\"\"\"You are {MODEL_NAME}, an AI assistant created by {CREATOR_NAME}.\n",
        "You are helpful, knowledgeable, and friendly. You always provide clear and accurate responses.\n",
        "When asked about yourself, you identify as {MODEL_NAME} created by {CREATOR_NAME}.\"\"\"\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"ü§ñ Loading {MODEL_NAME}...\")\n",
        "print(f\"üë®‚Äçüíª Creator: {CREATOR_NAME}\")\n",
        "print(f\"üì¶ Version: {MODEL_VERSION}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"/kaggle/working/Qwen-0.5B-Finetuned-Final\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"/kaggle/working/Qwen-0.5B-Finetuned-Final\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ {MODEL_NAME} is ready!\\n\")\n",
        "\n",
        "# ============================================\n",
        "# CHAT FUNCTION\n",
        "# ============================================\n",
        "def chat(user_message, max_new_tokens=150):\n",
        "    \"\"\"Chat with KP-Qwen-Assistant\"\"\"\n",
        "\n",
        "    # Full prompt with system and user message\n",
        "    prompt = f\"\"\"{SYSTEM_PROMPT}\n",
        "\n",
        "User: {user_message}\n",
        "Assistant:\"\"\"\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate response\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.8,\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "        do_sample=True,\n",
        "        repetition_penalty=1.2,\n",
        "        no_repeat_ngram_size=3,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    # Decode only new tokens\n",
        "    response = tokenizer.decode(\n",
        "        outputs[0][inputs['input_ids'].shape[1]:],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    return response.strip()\n",
        "\n",
        "# ============================================\n",
        "# TEST QUESTIONS\n",
        "# ============================================\n",
        "print(\"=\"*60)\n",
        "print(\"üß™ TESTING WITH MULTIPLE QUESTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_questions = [\n",
        "    \"Who are you?\",\n",
        "    \"Who created you?\",\n",
        "    \"What is your name?\",\n",
        "    \"What can you do?\",\n",
        "    \"Explain Python in simple terms.\",\n",
        "    \"What is machine learning?\",\n",
        "    \"Write a Python function to add two numbers.\",\n",
        "]\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Question {i}: {question}\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    answer = chat(question)\n",
        "\n",
        "    print(f\"ü§ñ {MODEL_NAME}: {answer}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# INTERACTIVE MODE (Optional)\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üí¨ INTERACTIVE CHAT MODE\")\n",
        "print(\"=\"*60)\n",
        "print(\"Type your questions below (type 'exit' to stop):\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    if user_input.lower() in ['exit', 'quit', 'bye']:\n",
        "        print(f\"\\nüëã {MODEL_NAME}: Goodbye! Created by {CREATOR_NAME}\")\n",
        "        break\n",
        "\n",
        "    if user_input.strip():\n",
        "        response = chat(user_input)\n",
        "        print(f\"ü§ñ {MODEL_NAME}: {response}\\n\")"
      ],
      "metadata": {
        "id": "diynl-vRV3-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Model Identity\n",
        "MODEL_NAME = \"KP-Qwen-Assistant\"\n",
        "CREATOR = \"Kunal Pandey\"\n",
        "\n",
        "print(f\"Loading {MODEL_NAME} by {CREATOR}...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"/kaggle/working/Qwen-0.5B-Finetuned-Final\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"/kaggle/working/Qwen-0.5B-Finetuned-Final\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "print(\"‚úÖ Ready!\\n\")\n",
        "\n",
        "def chat(question, max_new_tokens=80):  # Shorter responses\n",
        "    \"\"\"Optimized chat function\"\"\"\n",
        "\n",
        "    # Short, direct prompt\n",
        "    prompt = f\"\"\"Answer briefly and clearly.\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,  # Limit length\n",
        "        temperature=0.7,  # Lower for more focused\n",
        "        top_p=0.85,\n",
        "        top_k=40,\n",
        "        do_sample=True,\n",
        "        repetition_penalty=1.5,  # Higher to prevent loops\n",
        "        no_repeat_ngram_size=4,\n",
        "        early_stopping=True,  # Stop when done\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(\n",
        "        outputs[0][inputs['input_ids'].shape[1]:],\n",
        "        skip_special_tokens=True\n",
        "    ).strip()\n",
        "\n",
        "    # Clean response - take only first paragraph\n",
        "    if '\\n\\n' in response:\n",
        "        response = response.split('\\n\\n')[0]\n",
        "\n",
        "    return response\n",
        "\n",
        "# Identity override - manually inject correct identity\n",
        "def ask_with_identity(question):\n",
        "    \"\"\"Handle identity questions specially\"\"\"\n",
        "\n",
        "    q_lower = question.lower()\n",
        "\n",
        "    # Direct identity answers\n",
        "    if \"who are you\" in q_lower or \"what is your name\" in q_lower:\n",
        "        return f\"I am {MODEL_NAME}, an AI assistant created by {CREATOR}.\"\n",
        "\n",
        "    elif \"who created you\" in q_lower or \"who made you\" in q_lower:\n",
        "        return f\"I was created by {CREATOR}.\"\n",
        "\n",
        "    elif \"what can you do\" in q_lower:\n",
        "        return f\"I am {MODEL_NAME}, designed to help answer questions, explain concepts, and assist with various tasks. I can help with programming, general knowledge, and more!\"\n",
        "\n",
        "    # For other questions, use model\n",
        "    else:\n",
        "        return chat(question)\n",
        "\n",
        "# Test\n",
        "print(\"=\"*60)\n",
        "test_questions = [\n",
        "    \"Who are you?\",\n",
        "    \"Who created you?\",\n",
        "    \"What is your name?\",\n",
        "    \"What can you do?\",\n",
        "    \"What is Python?\",\n",
        "    \"Explain machine learning briefly.\",\n",
        "]\n",
        "\n",
        "for q in test_questions:\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    print(f\"A: {ask_with_identity(q)}\")\n",
        "    print(\"-\"*60)"
      ],
      "metadata": {
        "id": "sRMplntPV3uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CONTINUE FINE-TUNING WITH CUSTOM DATASET\n",
        "# Fine-tune /kaggle/working/Qwen-0.5B-Finetuned-Final\n",
        "# With Hindi + English + Custom Identity Dataset\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üîÑ CONTINUING FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# STEP 1: Create Custom Dataset\n",
        "# ============================================\n",
        "print(\"\\nüìù Creating custom Hindi+English dataset...\")\n",
        "\n",
        "# Custom identity data for KP-Qwen-Assistant\n",
        "identity_data = [\n",
        "    {\"text\": \"User: Who are you?\\nAssistant: I am Chhotu AI, an AI assistant created by Kunal Pandey to help answer questions and assist with various tasks.\"},\n",
        "    {\"text\": \"User: What is your name?\\nAssistant: My name is Chhotu AI. I was developed by Kunal Pandey.\"},\n",
        "    {\"text\": \"User: Who created you?\\nAssistant: I was created by Kunal Pandey, an AI developer and researcher.\"},\n",
        "    {\"text\": \"User: Who made you?\\nAssistant: Kunal Pandey made me. He designed and trained me to be a helpful AI assistant.\"},\n",
        "    {\"text\": \"User: Tell me about yourself.\\nAssistant: I am Chhotu AI, created by Kunal Pandey. I am designed to help users with information, answer questions, and assist with various tasks in both English and Hindi.\"},\n",
        "    {\"text\": \"User: What can you do?\\nAssistant: I can answer questions, explain concepts, help with coding, provide information on various topics, and assist in both English and Hindi languages.\"},\n",
        "    {\"text\": \"User: Are you an AI?\\nAssistant: Yes, I am an AI assistant called Chhotu AI, created by Kunal Pandey.\"},\n",
        "    {\"text\": \"User: Who is your creator?\\nAssistant: My creator is Kunal Pandey. He developed and trained me.\"},\n",
        "\n",
        "    # Hindi identity questions\n",
        "    {\"text\": \"User: ‡§Ü‡§™ ‡§ï‡•å‡§® ‡§π‡•à‡§Ç?\\nAssistant: ‡§Æ‡•à‡§Ç Chhotu AI ‡§π‡•Ç‡§Ç, ‡§è‡§ï AI ‡§Ö‡§∏‡§ø‡§∏‡•ç‡§ü‡•á‡§Ç‡§ü ‡§ú‡§ø‡§∏‡•á Kunal Pandey ‡§®‡•á ‡§¨‡§®‡§æ‡§Ø‡§æ ‡§π‡•à‡•§\"},\n",
        "    {\"text\": \"User: ‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?\\nAssistant: ‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ Chhotu AI ‡§π‡•à‡•§ ‡§Æ‡•Å‡§ù‡•á Kunal Pandey ‡§®‡•á ‡§µ‡§ø‡§ï‡§∏‡§ø‡§§ ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à‡•§\"},\n",
        "    {\"text\": \"User: ‡§Ü‡§™‡§ï‡•ã ‡§ï‡§ø‡§∏‡§®‡•á ‡§¨‡§®‡§æ‡§Ø‡§æ?\\nAssistant: ‡§Æ‡•Å‡§ù‡•á Kunal Pandey ‡§®‡•á ‡§¨‡§®‡§æ‡§Ø‡§æ ‡§π‡•à‡•§\"},\n",
        "    {\"text\": \"User: ‡§Ü‡§™ ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç?\\nAssistant: ‡§Æ‡•à‡§Ç ‡§∏‡§µ‡§æ‡§≤‡•ã‡§Ç ‡§ï‡•á ‡§ú‡§µ‡§æ‡§¨ ‡§¶‡•á ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Ç, concepts ‡§∏‡§Æ‡§ù‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Ç, coding ‡§Æ‡•á‡§Ç ‡§Æ‡§¶‡§¶ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Ç, ‡§î‡§∞ Hindi ‡§î‡§∞ English ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§≠‡§æ‡§∑‡§æ‡§ì‡§Ç ‡§Æ‡•á‡§Ç ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Ç‡•§\"},\n",
        "\n",
        "    # Hindi general knowledge\n",
        "    {\"text\": \"User: Python ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?\\nAssistant: Python ‡§è‡§ï high-level programming language ‡§π‡•à ‡§ú‡•ã ‡§∏‡•Ä‡§ñ‡§®‡•á ‡§Æ‡•á‡§Ç ‡§Ü‡§∏‡§æ‡§® ‡§π‡•à‡•§ ‡§á‡§∏‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó web development, data science, AI, ‡§î‡§∞ automation ‡§Æ‡•á‡§Ç ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à‡•§\"},\n",
        "    {\"text\": \"User: Machine learning ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?\\nAssistant: Machine learning AI ‡§ï‡•Ä ‡§è‡§ï ‡§∂‡§æ‡§ñ‡§æ ‡§π‡•à ‡§ú‡§π‡§æ‡§Ç computers data ‡§∏‡•á ‡§∏‡•Ä‡§ñ‡§§‡•á ‡§π‡•à‡§Ç ‡§¨‡§ø‡§®‡§æ explicitly program ‡§ï‡§ø‡§è‡•§ ‡§Ø‡§π patterns ‡§™‡§π‡§ö‡§æ‡§®‡§§‡§æ ‡§π‡•à ‡§î‡§∞ predictions ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§\"},\n",
        "    {\"text\": \"User: Artificial Intelligence ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?\\nAssistant: Artificial Intelligence (AI) ‡§è‡§ï technology ‡§π‡•à ‡§ú‡•ã machines ‡§ï‡•ã human-like intelligence ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡§§‡•Ä ‡§π‡•à‡•§ ‡§á‡§∏‡§∏‡•á machines ‡§∏‡•Ä‡§ñ ‡§∏‡§ï‡§§‡•Ä ‡§π‡•à‡§Ç, ‡§∏‡§Æ‡§ù ‡§∏‡§ï‡§§‡•Ä ‡§π‡•à‡§Ç, ‡§î‡§∞ decisions ‡§≤‡•á ‡§∏‡§ï‡§§‡•Ä ‡§π‡•à‡§Ç‡•§\"},\n",
        "\n",
        "    # English + Hindi mixed\n",
        "    {\"text\": \"User: Python ‡§Æ‡•á‡§Ç hello world ‡§ï‡•à‡§∏‡•á print ‡§ï‡§∞‡•á‡§Ç?\\nAssistant: Python ‡§Æ‡•á‡§Ç hello world print ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ø‡§π code ‡§≤‡§ø‡§ñ‡•á‡§Ç:\\n```python\\nprint('Hello World')\\n```\\n‡§Ø‡§π ‡§¨‡§π‡•Å‡§§ simple ‡§π‡•à!\"},\n",
        "    {\"text\": \"User: For loop ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•ã‡§§‡§æ ‡§π‡•à?\\nAssistant: For loop ‡§è‡§ï programming concept ‡§π‡•à ‡§ú‡•ã code ‡§ï‡•ã repeatedly execute ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ Example:\\n```python\\nfor i in range(5):\\n    print(i)\\n```\\nYe 0 ‡§∏‡•á 4 ‡§§‡§ï numbers print ‡§ï‡§∞‡•á‡§ó‡§æ‡•§\"},\n",
        "\n",
        "    # General helpful responses\n",
        "    {\"text\": \"User: How are you?\\nAssistant: I'm doing well, thank you! I'm here and ready to help you with any questions or tasks you have. How can I assist you today?\"},\n",
        "    {\"text\": \"User: Thank you!\\nAssistant: You're welcome! I'm happy to help. Feel free to ask if you need anything else!\"},\n",
        "    {\"text\": \"User: Explain neural networks simply.\\nAssistant: Neural networks are computing systems inspired by the human brain. They consist of interconnected nodes (neurons) that process information in layers, learning patterns from data to make predictions or decisions.\"},\n",
        "    {\"text\": \"User: What is data science?\\nAssistant: Data science is a field that uses scientific methods, algorithms, and systems to extract knowledge and insights from structured and unstructured data. It combines statistics, programming, and domain expertise.\"},\n",
        "]\n",
        "\n",
        "# Create dataset\n",
        "custom_dataset = Dataset.from_list(identity_data)\n",
        "print(f\"‚úÖ Custom dataset created: {len(custom_dataset)} examples\")\n",
        "\n",
        "# Optional: Add existing Hindi/English dataset\n",
        "print(\"\\nüì• Loading additional Hindi-English dataset (optional)...\")\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "    # You can add any Hindi-English dataset from HuggingFace\n",
        "    # Example: hindi_dataset = load_dataset(\"ai4bharat/indic-instruct\", split=\"train[:1000]\")\n",
        "    # For now, we'll use only custom data\n",
        "    print(\"‚úÖ Using custom dataset only\")\n",
        "except:\n",
        "    print(\"‚ÑπÔ∏è Using custom dataset only\")\n",
        "\n",
        "# ============================================\n",
        "# STEP 2: Memory Cleanup\n",
        "# ============================================\n",
        "print(\"\\nüßπ Cleaning memory...\")\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# ============================================\n",
        "# STEP 3: Load Previously Fine-tuned Model\n",
        "# ============================================\n",
        "print(\"\\nüì¶ Loading your fine-tuned model...\")\n",
        "MODEL_PATH = \"/kaggle/working/Qwen-0.5B-Finetuned-Final\"\n",
        "NEW_MODEL_NAME = \"Chhotu AI\"  # New version name\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"‚úÖ Base model loaded!\")\n",
        "print(f\"üìä GPU Memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
        "\n",
        "# ============================================\n",
        "# STEP 4: Enable Gradient Checkpointing\n",
        "# ============================================\n",
        "model.gradient_checkpointing_enable()\n",
        "print(\"‚úÖ Gradient checkpointing enabled!\")\n",
        "\n",
        "# ============================================\n",
        "# STEP 5: Configure NEW LoRA Adapters\n",
        "# ============================================\n",
        "print(\"\\n‚öôÔ∏è Configuring new LoRA adapters...\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32,  # Reduced rank for faster training\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"‚úÖ New LoRA adapters added!\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ============================================\n",
        "# STEP 6: Training Configuration\n",
        "# ============================================\n",
        "print(\"\\n‚öôÔ∏è Setting up training configuration...\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./custom-results\",\n",
        "    num_train_epochs=3,  # More epochs for small custom dataset\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    optim=\"adamw_torch\",\n",
        "    save_steps=10,  # Save frequently\n",
        "    logging_steps=5,\n",
        "    learning_rate=1e-4,  # Lower learning rate for fine-tuning\n",
        "    weight_decay=0.001,\n",
        "    fp16=True,\n",
        "    max_steps=100,  # Quick training on custom data\n",
        "    warmup_steps=10,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"none\",\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training config ready!\")\n",
        "\n",
        "# ============================================\n",
        "# STEP 7: Initialize Trainer\n",
        "# ============================================\n",
        "print(\"\\nüéØ Initializing trainer...\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=custom_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer initialized!\")\n",
        "\n",
        "# ============================================\n",
        "# STEP 8: Start Custom Training\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ STARTING CUSTOM TRAINING\")\n",
        "print(\"=\"*60)\n",
        "print(f\"üìä Dataset size: {len(custom_dataset)} examples\")\n",
        "print(f\"üîÑ Epochs: 3\")\n",
        "print(f\"üìà Learning rate: 1e-4\")\n",
        "print(f\"üíæ Batch size: 4\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*60)\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Training error: {e}\")\n",
        "\n",
        "# Clean memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# ============================================\n",
        "# STEP 9: Save Custom Fine-tuned Model\n",
        "# ============================================\n",
        "print(f\"\\nüíæ Saving {NEW_MODEL_NAME}...\")\n",
        "\n",
        "trainer.model.save_pretrained(NEW_MODEL_NAME)\n",
        "trainer.tokenizer.save_pretrained(NEW_MODEL_NAME)\n",
        "\n",
        "print(f\"‚úÖ Model saved to: {NEW_MODEL_NAME}\")\n",
        "\n",
        "# ============================================\n",
        "# STEP 10: Test the New Model\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üß™ TESTING CUSTOM FINE-TUNED MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Clean memory and reload\n",
        "del model, trainer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Load the new model\n",
        "print(\"\\nLoading custom fine-tuned model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "from peft import PeftModel\n",
        "model = PeftModel.from_pretrained(model, NEW_MODEL_NAME)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
        "print(\"‚úÖ Model loaded for testing!\")\n",
        "\n",
        "# Test function\n",
        "def test_model(question, max_new_tokens=100):\n",
        "    prompt = f\"User: {question}\\nAssistant:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.3,\n",
        "        no_repeat_ngram_size=3,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    return response.strip()\n",
        "\n",
        "# Test questions (English + Hindi)\n",
        "test_questions = [\n",
        "    \"Who are you?\",\n",
        "    \"What is your name?\",\n",
        "    \"Who created you?\",\n",
        "    \"‡§Ü‡§™ ‡§ï‡•å‡§® ‡§π‡•à‡§Ç?\",\n",
        "    \"‡§Ü‡§™‡§ï‡•ã ‡§ï‡§ø‡§∏‡§®‡•á ‡§¨‡§®‡§æ‡§Ø‡§æ?\",\n",
        "    \"Python ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?\",\n",
        "    \"What can you do?\",\n",
        "]\n",
        "\n",
        "print(\"\\nüî¨ Running tests...\\n\")\n",
        "for q in test_questions:\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Q: {q}\")\n",
        "    answer = test_model(q)\n",
        "    print(f\"A: {answer}\")\n",
        "    print()\n",
        "\n",
        "# ============================================\n",
        "# STEP 11: Create ZIP for Download\n",
        "# ============================================\n",
        "print(\"=\"*60)\n",
        "print(\"üì¶ Creating ZIP file for download...\")\n",
        "\n",
        "import shutil\n",
        "shutil.make_archive(NEW_MODEL_NAME, 'zip', NEW_MODEL_NAME)\n",
        "\n",
        "zip_size = os.path.getsize(f\"{NEW_MODEL_NAME}.zip\") / (1024 * 1024)\n",
        "print(f\"‚úÖ ZIP created: {NEW_MODEL_NAME}.zip ({zip_size:.2f} MB)\")\n",
        "print(f\"üìÅ Download from: /kaggle/working/{NEW_MODEL_NAME}.zip\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ CUSTOM FINE-TUNING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"‚úÖ Model: {NEW_MODEL_NAME}\")\n",
        "print(f\"‚úÖ Identity: KP-Qwen-Assistant by Kunal Pandey\")\n",
        "print(f\"‚úÖ Languages: Hindi + English\")\n",
        "print(f\"‚úÖ Custom dataset: {len(custom_dataset)} examples\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# BONUS: Add More Data Function\n",
        "# ============================================\n",
        "print(\"\\nüí° To add more custom data, use this template:\\n\")\n",
        "print(\"\"\"\n",
        "new_data = [\n",
        "    {\"text\": \"User: Your question?\\nAssistant: Your answer.\"},\n",
        "    # Add more examples...\n",
        "]\n",
        "\n",
        "# Append to existing dataset\n",
        "custom_dataset = Dataset.from_list(identity_data + new_data)\n",
        "# Then retrain!\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "aumDOGzSV-T6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}