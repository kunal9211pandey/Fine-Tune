{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBVUPOezDRLk"
      },
      "outputs": [],
      "source": [
        "# ==================== INSTALLATION ====================\n",
        "print(\"Installing required packages...\")\n",
        "!pip install -q -U transformers accelerate datasets peft trl\n",
        "!pip install -q -U scipy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "Ir5rgUvDDVvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== MEMORY CLEANUP FUNCTION ====================\n",
        "def cleanup_memory():\n",
        "    \"\"\"Memory cleanup function for P100 GPU\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
        "        print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
        "\n",
        "# Initial cleanup\n",
        "cleanup_memory()"
      ],
      "metadata": {
        "id": "VEkLy4lKDVsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== MODEL & TOKENIZER SETUP ====================\n",
        "MODEL_NAME = \"ibm-granite/granite-4.0-h-350M\"\n",
        "print(f\"\\nLoading model: {MODEL_NAME}\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "ljsk3gU_DVox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== LOAD MODEL (NO QUANTIZATION) ====================\n",
        "# P100 GPU ke saath quantization me issue hai, isliye direct FP16 use karenge\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,  # FP16 for memory efficiency\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "# Model config\n",
        "model.config.use_cache = False\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "print(\"‚úì Model loaded successfully!\")\n",
        "cleanup_memory()"
      ],
      "metadata": {
        "id": "wJAz7FiQDVb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== LORA CONFIGURATION ====================\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # Reduced rank for P100\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "bPrj583mDVLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== DATASET LOADING ====================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Loading dataset...\")\n",
        "\n",
        "# Using Alpaca dataset (smaller subset)\n",
        "try:\n",
        "    dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:2000]\")\n",
        "    print(f\"‚úì Dataset loaded: {len(dataset)} examples\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    print(\"Trying alternative dataset...\")\n",
        "    dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train[:2000]\")\n"
      ],
      "metadata": {
        "id": "PFWSh3GcDx1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== DATA PREPROCESSING ====================\n",
        "def format_instruction(example):\n",
        "    \"\"\"Format data in instruction-response format\"\"\"\n",
        "    instruction = example.get(\"instruction\", \"\")\n",
        "    input_text = example.get(\"input\", \"\")\n",
        "    output = example.get(\"output\", \"\")\n",
        "\n",
        "    if input_text:\n",
        "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output}\"\n",
        "    else:\n",
        "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}\"\n",
        "\n",
        "    return {\"text\": prompt}\n",
        "\n",
        "# Format dataset\n",
        "dataset = dataset.map(format_instruction, remove_columns=dataset.column_names)\n"
      ],
      "metadata": {
        "id": "3mXRwq_YDxmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=256,  # Reduced for P100\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None\n",
        "    )\n",
        "\n",
        "print(\"Tokenizing dataset...\")\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names,\n",
        "    desc=\"Tokenizing\"\n",
        ")"
      ],
      "metadata": {
        "id": "_n4DaV3hD4Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train/eval\n",
        "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "eval_dataset = split_dataset[\"test\"]\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Eval samples: {len(eval_dataset)}\")\n",
        "\n",
        "cleanup_memory()"
      ],
      "metadata": {
        "id": "BwUuiM3OD4MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== TRAINING CONFIGURATION ====================\n",
        "OUTPUT_DIR = \"./granite-finetuned\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=1,  # 1 epoch for demo\n",
        "    per_device_train_batch_size=2,  # Slightly higher batch\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=8,  # Effective batch = 16\n",
        "    warmup_steps=30,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,  # FP16 for P100\n",
        "    logging_steps=20,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=False,  # Disable to save memory\n",
        "    report_to=\"none\",\n",
        "    gradient_checkpointing=True,\n",
        "    max_grad_norm=0.3,\n",
        "    remove_unused_columns=False,\n",
        "    ddp_find_unused_parameters=False,\n",
        "    dataloader_pin_memory=False  # Reduce memory usage\n",
        ")"
      ],
      "metadata": {
        "id": "4vdNphrtECdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")"
      ],
      "metadata": {
        "id": "pvtf4O0mECPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== TRAINER ====================\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "OefX8XcoECCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== TRAINING ====================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Starting training...\")\n",
        "print(\"=\"*50)\n",
        "cleanup_memory()\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"\\n‚úì Training completed successfully!\")\n",
        "\n",
        "    # Save final model\n",
        "    trainer.save_model(OUTPUT_DIR)\n",
        "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "    print(f\"‚úì Model saved to {OUTPUT_DIR}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Training error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    cleanup_memory()\n",
        "    raise\n",
        "\n",
        "# ==================== FINAL CLEANUP ====================\n",
        "cleanup_memory()\n"
      ],
      "metadata": {
        "id": "1AApDfOREdqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== INFERENCE TEST ====================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Testing fine-tuned model...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "-Abz-pXtEhL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"### Instruction:\\nWhat is machine learning?\\n\\n### Response:\",\n",
        "    \"### Instruction:\\nWrite a short poem about technology\\n\\n### Response:\"\n",
        "]\n",
        "\n",
        "for i, test_prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\n--- Test {i} ---\")\n",
        "    print(f\"Prompt: {test_prompt.split('Response:')[0]}Response:\")\n",
        "\n",
        "    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=60,\n",
        "            temperature=0.8,\n",
        "            do_sample=True,\n",
        "            top_p=0.92,\n",
        "            top_k=50,\n",
        "            repetition_penalty=1.15,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    response = generated_text.split(\"### Response:\")[-1].strip()\n",
        "    print(f\"Response: {response}\\n\")\n",
        "\n",
        "# ==================== SAVE MERGED MODEL ====================\n",
        "print(\"=\"*50)\n",
        "print(\"Merging LoRA weights for deployment...\")\n",
        "\n",
        "try:\n",
        "    merged_model = model.merge_and_unload()\n",
        "    merged_model.save_pretrained(f\"{OUTPUT_DIR}/merged_model\")\n",
        "    tokenizer.save_pretrained(f\"{OUTPUT_DIR}/merged_model\")\n",
        "    print(f\"‚úì Merged model saved to {OUTPUT_DIR}/merged_model\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: Could not merge model (optional): {e}\")\n",
        "\n",
        "# Final cleanup\n",
        "cleanup_memory()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úì‚úì‚úì Fine-tuning pipeline completed successfully! ‚úì‚úì‚úì\")\n",
        "print(\"=\"*50)\n",
        "print(f\"\\nModel files location: {OUTPUT_DIR}\")\n",
        "print(\"\\nYou can now use this model for inference!\")\n"
      ],
      "metadata": {
        "id": "SRO2aViKEpFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== OPTIONAL: Test merged model ====================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Testing merged model (if available)...\")\n",
        "try:\n",
        "    test_model = AutoModelForCausalLM.from_pretrained(\n",
        "        f\"{OUTPUT_DIR}/merged_model\",\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    test_tokenizer = AutoTokenizer.from_pretrained(f\"{OUTPUT_DIR}/merged_model\")\n",
        "\n",
        "    test_input = \"### Instruction:\\nExplain AI in simple words\\n\\n### Response:\"\n",
        "    inputs = test_tokenizer(test_input, return_tensors=\"pt\").to(test_model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = test_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            temperature=0.7,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "    result = test_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"\\nMerged model output:\\n{result.split('Response:')[-1].strip()}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Merged model test skipped: {e}\")\n",
        "\n",
        "print(\"\\nüéâ All done!\")"
      ],
      "metadata": {
        "id": "VgzwZMN9E17P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}